{"cells":[{"metadata":{"_uuid":"a2da91ae854c355b8a810c44027f9f543a4053f6"},"cell_type":"markdown","source":"**Donor Recommender System (Content Based Recommendation)**\n\nIn this kernel I won't be doing EDA for data since there are already excellent kernels with some valuable insights. I would straight away jump to developing recommender system. \nTo reiterate, the problem statement is : **Using all the previous donations, projects and donors details build a system that can suggest donors that are most likely to donate for any given new project.**\nInitially I would be using project categories along with word vectors to suggest donors. \n**Word Vectors** are vector representation of words that is generated by an algorithm which trains on large corpus of text. Each word has it's own vector of dimension 100,200 or however we choos it to be. Main advantage of this is that words that are semantically similar or appear in text closely and closer to each other in vector space. Like if we build our of word vectors by training on large number of documents then vectors representing words like 'book' and 'author' would be very similar. We will exploit this to our advantage as you will see later. Since gathering such large text corpus, preprocessing and training is a time consuming task, and we already have pre-trained models available already we would be using those. Here I will be using Glove vectors which can be found in Kaggle also\n\n1. Build Word2Vec using pre-trained Glove vectors\n2. Calculate average word vectors for all the unique categories used in the projects dataset and store it a list.\n3. Given any new project, extract it's category, find the average word vector and find most similar vector from above list.\n4. One you have most similar categories, get projects tagged under those categories.\n5. Finally get donors who have donated to projects found above.\n\nLet's begin with imports and initializations :\n"},{"metadata":{"trusted":true,"_uuid":"c72514c44b5cd661d1451f7d0ed27b343b04d7ae"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport os\nimport pickle\nimport math\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import text\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# stop words are not used in the kernel anymore since I am using pre-trained model.\nstop = set(stopwords.words('english'))\nstop = stop.union(set(string.punctuation))\nstop = text.ENGLISH_STOP_WORDS.union(stop)\n\ntranslator = str.maketrans('', '', string.punctuation)\n\nglove_input_file = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt' \nword2vec_output_file = 'glove_w2v.txt'\nprojects_file = \"projects\"\nmodel_file = 'model'\nupdate = False # if for some reason you want to update the loaded objects make it True\n# ['glove.6B.50d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.6B.100d.txt'] these are the available pre-trained models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b8315f501c5b52c7828e1cdd067c1bae02de222"},"cell_type":"markdown","source":"Defining some functions that I am going to use. Methods are pretty simple with comments so I won't explain here again."},{"metadata":{"trusted":true,"_uuid":"cc32546632725624a31d1574919fb5c29ffaed43"},"cell_type":"code","source":"def print_time(tag, start_time):\n    \"\"\"\n    To print time difference with a text\n    \"\"\"\n    print(tag,(time.time()-start_time))\n    \ndef get_avg_vec(words):\n    \"\"\"\n    This method accepts a string with multiple words and returns the \n    average of there word vectors.\n    \"\"\"\n    avg = np.zeros((len(model[\"book\"]),))\n    for word in words.split():\n        try:\n            avg = avg + model[word.lower()]\n        except:\n            print(\"word not found\",word)\n\n    avg = avg/len(words.split())\n    if(np.isnan(avg).any()):\n        print(\"Nan\",words,len(words.split()))\n    return avg\n\ndef smooth_donor_preference(x):\n    \"\"\"\n    To reduce large numbers to smaller, comparable values. I had found this idea in one of the kernels some time back.\n    I do not take credit for this smoothening idea.\n    \"\"\"\n    return math.log(1+x, 2)\n\ndef build_df_groupy_donor(df):\n    df[\"eventStrength\"] = df[\"Donation Amount\"]\n    return df.groupby(['Project ID','Donor ID'])['eventStrength'].sum().apply(smooth_donor_preference).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"e450accb92f564afd15fa5b38dcc4ae053b02009"},"cell_type":"code","source":"print(os.listdir(\"../input/io\"))\nprint(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c08d73fc55fbbfe5590e92a55fd989dbc5f53b9"},"cell_type":"markdown","source":"Build a word2vec model from glove vectors file. We would using file with 200 dimenstion vectors. Once model is built for the first time, store it in output storage for loading it directly for all subsequent runs."},{"metadata":{"trusted":true,"_uuid":"3da34f2b400437eb3dae93e6b6c89b22db169a31"},"cell_type":"code","source":"model = None\nif(not os.path.isfile(word2vec_output_file) or update):\n    print(\"Glove model not pre-loaded. Loading now...\")\n    glove2word2vec(glove_input_file, word2vec_output_file)\n    load_time = time.time()\n    model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n    print_time(\"model load time\",load_time)\n    pickle.dump(model,open(model_file,'wb')) # store the word2vec model in output folder after building for first time to save time\n\nif(model is None): # if model is already available in output just load it\n    load_time = time.time()\n    model = pickle.load(open(model_file,'rb'))\n    print_time(\"model pickle load time\",load_time)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3656fe649d538d82c26c6d4ff328fde075c2c603"},"cell_type":"markdown","source":"Now let's test if it's actually working."},{"metadata":{"trusted":true,"_uuid":"8a5684ded8e64db16dfeb129f4c10bd1f0222c08"},"cell_type":"code","source":"print(\"size of output file\",os.path.getsize(word2vec_output_file)//(1024*1024),\"MB\")\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) # just to check if model is actually working\nprint(\"Testing Word2Vec model (Result should be 'queen')\",result)\nprint(model.most_similar(positive=[\"book\"],topn=2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a3a16310ae13c1c6f701c628b675b4193a3542"},"cell_type":"markdown","source":"Now read projects and donations CSV and store them as DataFrames. We keep only 50K rows for projects here for saving memory. Otherwise some times kernel stops working."},{"metadata":{"trusted":true,"_uuid":"f36a9fbe4a1461d6b659faa4dc32280f545430db"},"cell_type":"code","source":"df_projects = None\nif(not os.path.isfile(projects_file) or update):\n    print(\"No pre-loaded projects found. Loading now...\")\n    df_projects = pd.read_csv(\"../input/io/Projects.csv\", low_memory=False)\n    df_projects = df_projects[0:50000] # reading only 50K rows to save time\n    pickle.dump(df_projects,open(projects_file,'wb'))\n\nprint(\"size of projects file\",os.path.getsize(projects_file)//(1024*1024),\"MB\")\nif(df_projects is None): # if already in storage, load it.\n    df_projects = pickle.load(open(projects_file,'rb'))\n\n# drop projects with no category\ndf_projects.dropna(subset=['Project Subject Category Tree'],inplace=True)\ncategories = df_projects[\"Project Subject Category Tree\"].unique()\nprint(\"These are the overall categories in use till now\")\nprint(categories)\n\nload_time = time.time()\ndf_donations = pd.read_csv(\"../input/io/Donations.csv\", low_memory=False)\ndf_gp = build_df_groupy_donor(df_donations)\n#df_gp.set_index(\"Project ID\",inplace=True)\ndisplay(df_gp[0:50])\nprint_time(\"Donations load time\",load_time)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5e5e6570c2491a6fa243307481466320cf6029"},"cell_type":"markdown","source":"Now that loading (boring) part is over let's understand how we are going to recommend donors. The idea is to use the categories (\"Project Subject Category Tree\") tagged to each project to find similar projects. While I could have simply matched them directly (Like Project ID == \"Math & Science\"), that wouldn't be effective in longer run since someone could write \"math & science\" as \"Math and Science\". Also another advantage is that even if a new category is introduced, this system would work by returning most similar category. For example if new project is tagged under \"Fitness\" it would return projects with \"Health & Sports\", like we would see later in action. Awsome! right? \nAlso the idea of using NLP techniques to find similar projects started with trying to get projec type based on \"Project Essay\". I tried multiple ways like TF-IDF with cosine similarity, TF-IDF & word vectors with K-Means etc but results were not satisfactory, may be because \"Project Essay\" has a lot of words that don't really add any value to project type. But it may achievable by tweaking idf parameters to remove useless words. Here is a sample essay to show insignificant words :"},{"metadata":{"trusted":true,"_uuid":"ccbfcbc5fe1fb463a109bcaf02ccdcc79d334c12"},"cell_type":"code","source":"#display(df_projects[df_projects[\"Project Subject Category Tree\"] == 'Math & Science, Literacy & Language'].head())\ntext_ms_ll = df_projects[df_projects[\"Project Subject Category Tree\"] == 'Math & Science, Literacy & Language'].iloc[0][\"Project Essay\"]\nprint(text_ms_ll)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"263b198bb4084462d1b6660edc48421862fb5121"},"cell_type":"markdown","source":"This is the final piece of code. In the below code **translator** is something which removes all the punctuations from a given string \"Math & Science\" would be converted to \"Math Science\" so that we can calculating accurate word vec averages. As we have already discussed, even if a new project with unseen category is passed to this system it would spit out the most similar categories gracefully. You see it in the output of next cell."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"stime = time.time()\ncat_vec = np.zeros((len(categories),len(model[\"book\"])))\nprint(\"Number of categories\",len(categories))\ncount = 0\n\"\"\"\n'categories' holds all the unique categories used in projects dataset.\nwe calculate average word vectors for each of them and store in 'cat_vec'\n\"\"\"\nfor cat in categories:\n    #print(\"Real-\",cat.strip(),\"--trans-\",cat.strip().translate(translator))\n    words = cat.strip().translate(translator)\n    cat_vec[count] = get_avg_vec(words)\n    count = count + 1\n\nprint(\"category vectors calculated\",cat_vec.shape)\n\ndef get_similar_category(cats):\n    \"\"\"\n    parameter : 'n' category strings\n    returns : list of most similar category (from projects dataset) to each of the 'n' categories sent as parameter\n    \"\"\"\n    #arr.argsort()[-3:]\n    sim_cats = []\n    for cat in cats:\n        words = cat.strip().translate(translator)\n        avg = get_avg_vec(words)\n        res = cosine_similarity(cat_vec,avg.reshape(1,-1))\n        max_ind = np.argmax(res)\n        print(\"Given category- \",cat,\", Most similar category- \",categories[max_ind])\n        sim_cats.append(categories[max_ind])\n    return sim_cats\n\nres = get_similar_category([\"Applied Math\",\"Fitness\"]) # array of matching categories\nprint(\"\\n\")\nfor cat in res:\n    print(\"For category\",cat)\n    df_temp = df_projects[df_projects[\"Project Subject Category Tree\"] == cat].reset_index() # keep only fields needed\n    #display(df_temp)\n    df_cat_projs = df_gp[df_gp[\"Project ID\"].isin(df_temp[\"Project ID\"])]#.sort_values(\"eventStrength\",ascending=False)\n    df_cat_projs = df_cat_projs.groupby([\"Project ID\",\"Donor ID\"]).agg({'eventStrength':sum})\n    df_cat_projs.sort_values(\"eventStrength\",ascending=False,inplace=True)\n    # suggest donors who have donated most genorously for better turnover and brevity of display\n    df_cat_projs = df_cat_projs[df_cat_projs[\"eventStrength\"] > 7]\n    print(\"Suggested donors ({0}) for category {1}\".format(len(df_cat_projs),cat))\n    display(df_cat_projs)\n\nprint_time(\"End time\",stime)\n# 'Math & Science, Literacy & Language', 'Health & Sports, History & Civics'\n#next think about collborative filtering with matrix [projects, donors]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Things in pipeline : \n1. Using other features to use while suggesting. Like project location as people tend to support a charity more genorously in there state or locality.\nSo combining \"Donor Zip\" and Project Zip code.\n2. Trying collaborative filtering."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}